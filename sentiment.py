#----------------------------------------------------------------------------------------------------
#### XÃ‚Y Dá»°NG GUI : Sentiment Analysis Application
#### Äá»’ ÃN TN : Data Science 
#----------------------------------------------------------------------------------------------------
# import libraries
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import joblib
import streamlit as st
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import io
#----------------------------------------------------------------------------------------------------
# Custom CSS and Styling
#----------------------------------------------------------------------------------------------------
st.markdown("""
    <style>
    body {
        background-color: #F7F8FA;
        font-family: 'Roboto', sans-serif;
    }
    .stApp {
        background-color: #FFFFFF;
        border-radius: 15px;
        padding: 10px;
    }
    .css-1aumxhk {  /* Sidebar */
        background-color: #E8F5E9;
        font-family: 'Georgia', serif;
    }
    h1, h2, h3 {
        color: #4CAF50;
        font-weight: bold;
    }
    .stDownloadButton {
        font-size: 18px;
        font-weight: bold;
        background-color: #4CAF50;
        color: white;
    }
    .st-bar-chart {
        background-color: #E3F2FD;
    }
    .stButton > button {
        font-size: 16px;
        font-weight: bold;
        color: white;
        background-color: #4CAF50;
        border-radius: 10px;
        border: none;
        padding: 10px 20px;
    }
    </style>
""", unsafe_allow_html=True)
#----------------------------------------------------------------------------------------------------
# Part 1: Load Data and Models
#----------------------------------------------------------------------------------------------------
@st.cache_data
def load_data():
    """Load review dataset."""
    return pd.read_csv('valid_reviews.csv')

@st.cache_resource
def load_model_and_vectorizer():
    """Load pretrained model and CountVectorizer."""
    model = joblib.load('random_forest_model.pkl')  # Load trained model
    count_vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Load CountVectorizer
    return model, count_vectorizer

# Load data and models
valid_reviews = load_data()
model, count_vectorizer = load_model_and_vectorizer()

# Load files
emoji_file = 'emojicon.txt'
teencode_file = 'teencode.txt'
english_file = 'english-vnmese.txt'
wrong_word_file = 'wrong-word.txt'
stopwords_file = 'vietnamese-stopwords.txt'

# Äá»c dá»¯ liá»‡u tá»« cÃ¡c file
with open(emoji_file, 'r', encoding="utf8") as file:
    emoji_dict = {line.split('\t')[0]: line.split('\t')[1].strip() for line in file if '\t' in line}

with open(teencode_file, 'r', encoding="utf8") as file:
    teencode_dict = {line.split('\t')[0]: line.split('\t')[1].strip() for line in file if '\t' in line}

with open(english_file, 'r', encoding="utf8") as file:
    english_dict = {line.split('\t')[0]: line.split('\t')[1].strip() for line in file if '\t' in line}

with open(wrong_word_file, 'r', encoding="utf8") as file:
    wrong_words = set(file.read().splitlines())

with open(stopwords_file, 'r', encoding="utf8") as file:
    stopwords = set(file.read().splitlines())
#----------------------------------------------------------------------------------------------------
# Part 2: Text Preprocessing and Sentiment Analysis
#----------------------------------------------------------------------------------------------------
import re
import regex 
from underthesea import word_tokenize, pos_tag, sent_tokenize
# Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t
def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic
# ÄÆ°a toÃ n bá»™ dá»¯ liá»‡u qua hÃ m nÃ y Ä‘á»ƒ chuáº©n hÃ³a láº¡i
def covert_unicode(text):
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], text)
# HÃ m chuáº©n hÃ³a cÃ¡c tá»« cÃ³ kÃ½ tá»± láº·p
def normalize_repeated_characters(text):
    return regex.sub(r'(.)\1+', r'\1', text)
# HÃ m xá»­ lÃ½ emoji vá»›i dáº¥u cÃ¢u
def remove_emoji_punctuation(text):
    text = re.sub(r'([^\w\s])\.', r'\1', text)  # Xá»­ lÃ½ emoji + dáº¥u cÃ¢u, loáº¡i bá» dáº¥u cÃ¢u sau emoji náº¿u cáº§n
    return text
    
# HÃ m xá»­ lÃ½ emoji, teen code, vÃ  tá»« sai
def process_text(
    text, 
    emoji_dict, 
    teencode_dict, 
    wrong_words):
    """
    Process text by handling emojis, teen code, and removing unwanted characters.
    Preserve specific words in positive, neutral, and negative word lists.
    """
    document = text.lower()
    document = document.replace("â€™", '')  # Remove unwanted characters
    document = regex.sub(r'\.+', ".", document)  # Normalize dots
    new_sentence = ''
    
    for sentence in sent_tokenize(document):
        # Step 1: Handle emoji
        sentence = ' '.join(emoji_dict[word] if word in emoji_dict else word for word in sentence.split())
        
        # Step 2: Handle teencode
        sentence = ' '.join(teencode_dict[word] if word in teencode_dict else word for word in sentence.split())
        
        # Step 3: Extract only valid words
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern, sentence))
        
        # Step 4: Remove wrong words
        sentence = ' '.join('' if word in wrong_words else word for word in sentence.split())
        
        new_sentence += sentence + '. '  # Recombine sentences
    
        # Step 5: Remove excess whitespace
        document = regex.sub(r'\s+', ' ', new_sentence).strip()  
    return document

# HÃ m xá»­ lÃ½ tá»« Ä‘áº·c biá»‡t
def process_special_word(text):
    # cÃ³ thá»ƒ cÃ³ nhiá»u tá»« Ä‘áº·c biá»‡t cáº§n rÃ¡p láº¡i vá»›i nhau
    new_text = ''
    text_lst = text.split()
    i= 0
    # khÃ´ng, cháº³ng, cháº£...
    if 'khÃ´ng'or 'cháº³ng' or 'cháº£' or 'kÃ©m' in text_lst:
        while i <= len(text_lst) - 1:
            word = text_lst[i]
            #print(word)
            #print(i)
            if  word == 'khÃ´ng' or word == 'cháº³ng' or word == 'cháº£' or word == 'kÃ©m' :
                next_idx = i+1
                if next_idx <= len(text_lst) -1:
                    word = word +'_'+ text_lst[next_idx]
                i= next_idx + 1
            else:
                i = i+1
            new_text = new_text + word + ' '
    else:
        new_text = text
    return new_text.strip()

# HÃ m xá»­ lÃ½ POS tagging vÃ  lá»c tá»« loáº¡i
def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.','')
        ###### POS tag
        lst_word_type = ['N','Np','A','AB','V','R'] # Giá»›i háº¡n tá»« loáº¡i cáº§n thiáº¿t
        # lst_word_type = ['A','AB','V','VB','VY','R']
        sentence = ' '.join( word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        new_document = new_document + sentence + ' '
    ###### DEL excess blank space
    new_document = regex.sub(r'\s+', ' ', new_document).strip()
    return new_document
    
# HÃ m loáº¡i bá» tá»« dá»«ng
def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word not in stopwords]
    return " ".join(words)

# HÃ m tá»•ng há»£p tiá»n xá»­ lÃ½ Ä‘áº§y Ä‘á»§ bao gá»“m POS Tagging
def clean_text(text, emoji_dict, teencode_dict, wrong_words):
    text = normalize_repeated_characters(text)  # Chuáº©n hÃ³a kÃ½ tá»± láº·p
    text = covert_unicode(text)
    text = remove_emoji_punctuation(text)
    text = process_text(text, emoji_dict, teencode_dict, wrong_words)  # LÃ m sáº¡ch
    text = process_special_word(text)  # Xá»­ lÃ½ tá»« Ä‘áº·c biá»‡t
    text = process_postag_thesea(text)  # POS tagging vÃ  lá»c tá»« loáº¡i
    text = remove_stopwords(text)  # Loáº¡i bá» tá»« dá»«ng

    return text

def predict_sentiment(text, model, vectorizer):
    """
    Dá»± Ä‘oÃ¡n cáº£m xÃºc cá»§a vÄƒn báº£n Ä‘áº§u vÃ o.
    """
    try:
        # Xá»­ lÃ½ vÄƒn báº£n
        processed_text = clean_text(text, emoji_dict, teencode_dict, wrong_words)

        # Vector hÃ³a vÄƒn báº£n
        vectorized_text = vectorizer.transform([processed_text])

        # Dá»± Ä‘oÃ¡n
        prediction = model.predict(vectorized_text)
        print(f"Káº¿t quáº£ thÃ´ tá»« mÃ´ hÃ¬nh: {prediction}")  # Log giÃ¡ trá»‹ tráº£ vá» Ä‘á»ƒ kiá»ƒm tra

        # Ãnh xáº¡ nhÃ£n cáº£m xÃºc
        sentiment_mapping = {
            0: "TiÃªu cá»±c",
            1: "Trung láº­p",
            2: "TÃ­ch cá»±c",
            'negative': "TiÃªu cá»±c",
            'neutral': "Trung láº­p",
            'positive': "TÃ­ch cá»±c"
        }

        # Láº¥y nhÃ£n cáº£m xÃºc
        return sentiment_mapping.get(prediction[0], f"GiÃ¡ trá»‹ cáº£m xÃºc khÃ´ng mong Ä‘á»£i: {prediction[0]}")

    except Exception as e:
        print(f"Lá»—i trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n cáº£m xÃºc: {e}")
        return "Lá»—i trong dá»± Ä‘oÃ¡n"
#----------------------------------------------------------------------------------------------------
# Part 3: Build App
#----------------------------------------------------------------------------------------------------
# GUI
st.markdown("<h1 style='text-align: center; color: #4CAF50;'>ğŸŒŸ á»¨ng Dá»¥ng PhÃ¢n TÃ­ch Cáº£m XÃºc ğŸŒŸ</h1>", unsafe_allow_html=True)
st.markdown("<h3 style='text-align: center;'>PhÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡ khÃ¡ch hÃ ng Ä‘á»ƒ cáº£i thiá»‡n sáº£n pháº©m</h3>", unsafe_allow_html=True)

menu = st.sidebar.selectbox('Menu', ['Overview', 'Model Evaluation', 'Dá»± Ä‘oÃ¡n tá»« vÄƒn báº£n', 'Product ID Prediction'])
st.sidebar.write("""#### ThÃ nh viÃªn thá»±c hiá»‡n:
                 Pháº¡m Anh VÅ© & LÃ½ Quá»‘c Há»“ng PhÃºc""")
st.sidebar.image('phucly.png')
st.sidebar.image('vupham.jpg')
st.sidebar.write("""#### Giáº£ng viÃªn hÆ°á»›ng dáº«n: Khuáº¥t ThÃ¹y PhÆ°Æ¡ng""")
st.sidebar.image('khuat_thuy_phuong.jpg')
st.sidebar.write("""#### Thá»i gian thá»±c hiá»‡n: 05/12/2024""")
#----------------------------------------------------------------------------------------------------
if menu == 'Overview':
    st.subheader('Tá»•ng Quan Dá»± Ãn')
    st.image('Sentiment-Analysis.png')
    st.write("""
    **Objective**: Dá»± Ã¡n xoay quanh Hasaki.vn, má»™t cá»­a hÃ ng má»¹ pháº©m chuyÃªn cung cáº¥p cÃ¡c sáº£n pháº©m lÃ m Ä‘áº¹p Ä‘a dáº¡ng. Má»¥c tiÃªu chÃ­nh cá»§a dá»± Ã¡n lÃ  sá»­ dá»¥ng pháº£n há»“i tá»« khÃ¡ch hÃ ng Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng sáº£n pháº©m vÃ  dá»‹ch vá»¥. Cá»¥ thá»ƒ:
    1. PhÃ¢n tÃ­ch cÃ¡c Ä‘Ã¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh nhá»¯ng Ä‘iá»ƒm máº¡nh (pháº£n há»“i tÃ­ch cá»±c) vÃ  nhá»¯ng Ä‘iá»ƒm cáº§n cáº£i thiá»‡n (pháº£n há»“i tiÃªu cá»±c).

    2. Dá»±a vÃ o káº¿t quáº£ phÃ¢n tÃ­ch Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n vá» sá»Ÿ thÃ­ch cá»§a khÃ¡ch hÃ ng, kháº¯c phá»¥c cÃ¡c váº¥n Ä‘á» tá»“n táº¡i vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh dá»±a trÃªn dá»¯ liá»‡u Ä‘á»ƒ nÃ¢ng cao cháº¥t lÆ°á»£ng dá»‹ch vá»¥.

    """)
    st.image('Hasaki.logo.wide.jpg')
    st.write("""
    - **Key Features**: Lá»±a chá»n thuáº­t toÃ¡n phÃ¹ há»£p trong â€œMachine Learning with Pythonâ€ cho bÃ i toÃ¡n Sentiment classification (Positive, Neutral, Negative). Sá»­ dá»¥ng Random Forest.
    - **Analysis Tools**: Data cleaning, visualization, wordclouds, and machine learning models.
    - **ThÆ° viÃªn sá»­ dá»¥ng**:
- numpy, pandas, matplotlib, seaborn
- underthesea (Ä‘á»ƒ Ä‘á»c dá»¯ liá»‡u tiáº¿ng viá»‡t)
- worldcloud ( Ä‘á»ƒ thÃ nh 2 báº£ng, 1 lÃ  positive vá»›i cÃ¡c tá»« nhÆ° yÃªu thÃ­ch, tá»‘t... tiáº¿p lÃ  negative nhÆ° kÃ©m, tá»‡...)
- scikit-learn
- pyspark Ä‘á»ƒ dÃ¹ng machine learning,
    """)
    

#----------------------------------------------------------------------------------------------------
elif menu == 'Model Evaluation':
    st.subheader('Model Evaluation')
    st.write('### Valid Reviews Dataset sau khi Ä‘Ã£ tiá»n xá»­ lÃ½ vÃ  lÃ m sáº¡ch')
    st.table(valid_reviews.head())

    # Display sentiment distribution
    st.write('### PhÃ¢n phá»‘i Ä‘Ã¡nh giÃ¡ trÆ°á»›c khi xá»­ lÃ½')
    st.image('distribution.png')

    # WordClouds
    st.write('### WordClouds')
    st.image('positive.png')
    st.image('neutral.png')
    st.image('negative.png')

    
    st.write('#### CÃ¢n Báº±ng Dá»¯ Liá»‡u TrÆ°á»›c vÃ  Sau SMOTE')
    st.image('smote.png')
    st.write("""
    - **Äá»™ chÃ­nh xÃ¡c**: ~98%
    - **Äiá»ƒm máº¡nh**: Cao nháº¥t vá» Ä‘á»™ chÃ­nh xÃ¡c vÃ  hiá»‡u suáº¥t tá»‘t trÃªn má»i nhÃ£n. TÃ¡ch biá»‡t tá»‘t giá»¯a cÃ¡c lá»›p cáº£m xÃºc.
    - **CÃ¢n nháº¯c**:Thá»i gian xá»­ lÃ½ dÃ i hÆ¡n, yÃªu cáº§u tÃ i nguyÃªn tÃ­nh toÃ¡n cao hÆ¡n.
    - #### Káº¿t luáº­n: Random Forest lÃ  mÃ´ hÃ¬nh phÃ¹ há»£p nháº¥t, khuyáº¿n khÃ­ch sá»­ dá»¥ng cho dá»¯ liá»‡u nÃ y.""")
    st.image('evaluation.png')
    st.write('### Ma Tráº­n Nháº§m Láº«n')
    st.image('matrix.png')

#----------------------------------------------------------------------------------------------------
elif menu == 'Dá»± Ä‘oÃ¡n tá»« vÄƒn báº£n':
    st.subheader('Dá»± Ä‘oÃ¡n tá»« vÄƒn báº£n')
    st.write('Nháº­p vÄƒn báº£n Ä‘Ã¡nh giÃ¡ hoáº·c táº£i lÃªn tá»‡p CSV Ä‘á»ƒ há»‡ thá»‘ng dá»± Ä‘oÃ¡n cáº£m xÃºc.')

    choice = st.radio("Lá»±a chá»n phÆ°Æ¡ng thá»©c nháº­p liá»‡u:", ["Nháº­p vÄƒn báº£n", "Táº£i lÃªn tá»‡p CSV"])

    # Option 1: Text Input
    if choice == "Nháº­p vÄƒn báº£n":
        input_text = st.text_area("Nháº­p vÄƒn báº£n Ä‘Ã¡nh giÃ¡:")

        if st.button("Dá»± Ä‘oÃ¡n cáº£m xÃºc"):
            if input_text.strip():  # Kiá»ƒm tra xem vÄƒn báº£n cÃ³ trá»‘ng khÃ´ng
                try:
                    # Tiá»n xá»­ lÃ½ vÄƒn báº£n
                    processed_text = clean_text(
                        input_text, emoji_dict, teencode_dict, wrong_words)

                    # Hiá»ƒn thá»‹ vÄƒn báº£n sau khi tiá»n xá»­ lÃ½
                    st.write("**VÄƒn báº£n sau khi tiá»n xá»­ lÃ½:**")
                    st.code(processed_text)

                    # Chuyá»ƒn vÄƒn báº£n Ä‘Ã£ xá»­ lÃ½ thÃ nh vector
                    vectorized_text = count_vectorizer.transform([processed_text])

                    # Dá»± Ä‘oÃ¡n cáº£m xÃºc
                    prediction = model.predict(vectorized_text)

                    # Ãnh xáº¡ káº¿t quáº£ dá»± Ä‘oÃ¡n thÃ nh nhÃ£n cáº£m xÃºc
                    sentiment_mapping = {
            0: "TiÃªu cá»±c",
            1: "Trung láº­p",
            2: "TÃ­ch cá»±c",
            'negative': "TiÃªu cá»±c",
            'neutral': "Trung láº­p",
            'positive': "TÃ­ch cá»±c"
        }
                    sentiment = sentiment_mapping.get(prediction[0], "KhÃ´ng xÃ¡c Ä‘á»‹nh")

                    # Hiá»ƒn thá»‹ káº¿t quáº£ dá»± Ä‘oÃ¡n
                    st.write(f"**Káº¿t quáº£ dá»± Ä‘oÃ¡n:** {sentiment}")

                except Exception as e:
                    # Hiá»ƒn thá»‹ lá»—i náº¿u xáº£y ra trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n
                    st.error(f"Lá»—i trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n: {e}")
            else:
                # ThÃ´ng bÃ¡o náº¿u khÃ´ng cÃ³ vÄƒn báº£n nháº­p vÃ o
                st.warning("Vui lÃ²ng nháº­p vÄƒn báº£n Ä‘á»ƒ dá»± Ä‘oÃ¡n cáº£m xÃºc!")

    # Option 2: CSV File Upload
    elif choice == "Táº£i lÃªn tá»‡p CSV":
        uploaded_file = st.file_uploader("Chá»n tá»‡p CSV", type=["csv"])

        if uploaded_file is not None:
            try:
                # Äá»c tá»‡p CSV
                data = pd.read_csv(uploaded_file)

                # Kiá»ƒm tra cá»™t review_text cÃ³ tá»“n táº¡i
                if "noi_dung_binh_luan" not in data.columns:
                    st.error("Tá»‡p CSV pháº£i chá»©a cá»™t 'noi_dung_binh_luan'.")
                else:
                    # Tiá»n xá»­ lÃ½ vÄƒn báº£n
                    data["processed_text"] = data["noi_dung_binh_luan"].apply(
                        lambda x: clean_text(x, emoji_dict, teencode_dict, wrong_words)
                        if isinstance(x, str)
                        else None
                    )

                    # Hiá»ƒn thá»‹ vÄƒn báº£n Ä‘Ã£ qua tiá»n xá»­ lÃ½
                    st.write("**VÄƒn báº£n sau khi tiá»n xá»­ lÃ½:**")
                    st.dataframe(data[["noi_dung_binh_luan", "processed_text"]].head(10))

                    # Chuyá»ƒn vÄƒn báº£n Ä‘Ã£ xá»­ lÃ½ thÃ nh vector
                    vectorized_texts = count_vectorizer.transform(data["processed_text"].dropna())

                    # Dá»± Ä‘oÃ¡n cáº£m xÃºc
                    predictions = model.predict(vectorized_texts)

                    # Ãnh xáº¡ káº¿t quáº£ dá»± Ä‘oÃ¡n thÃ nh nhÃ£n cáº£m xÃºc
                    sentiment_mapping = {
            0: "TiÃªu cá»±c",
            1: "Trung láº­p",
            2: "TÃ­ch cá»±c",
            'negative': "TiÃªu cá»±c",
            'neutral': "Trung láº­p",
            'positive': "TÃ­ch cá»±c"
        }
                    data["sentiment"] = [
                        sentiment_mapping.get(pred, "KhÃ´ng xÃ¡c Ä‘á»‹nh") for pred in predictions
                    ]

                    # Hiá»ƒn thá»‹ dá»¯ liá»‡u vá»›i káº¿t quáº£ dá»± Ä‘oÃ¡n
                    st.write("**Káº¿t quáº£ dá»± Ä‘oÃ¡n:**")
                    st.dataframe(data[["noi_dung_binh_luan", "sentiment"]].head(10))

                    # Táº£i xuá»‘ng káº¿t quáº£
                    output = io.BytesIO()
                    data.to_csv(output, index=False)
                    output.seek(0)
                    st.download_button(
                        label="Táº£i xuá»‘ng káº¿t quáº£ dá»± Ä‘oÃ¡n",
                        data=output,
                        file_name="predictions.csv",
                        mime="text/csv",
                    )
            except Exception as e:
                st.error(f"Lá»—i trong quÃ¡ trÃ¬nh xá»­ lÃ½ tá»‡p CSV: {e}")

#----------------------------------------------------------------------------------------------------
elif menu == 'Product ID Prediction':
    st.subheader('Dá»± Ä‘oÃ¡n cáº£m xÃºc theo mÃ£ sáº£n pháº©m')
    st.write("PhÃ¢n tÃ­ch cáº£m xÃºc cá»§a khÃ¡ch hÃ ng cho má»™t hoáº·c nhiá»u mÃ£ sáº£n pháº©m.")

    choice = st.radio("Chá»n phÆ°Æ¡ng thá»©c nháº­p liá»‡u:", ["Nháº­p mÃ£ sáº£n pháº©m", "Táº£i lÃªn táº­p CSV"])

    if choice == "Nháº­p mÃ£ sáº£n pháº©m":
        product_id = st.text_input("Nháº­p mÃ£ sáº£n pháº©m:")
        if st.button("PhÃ¢n tÃ­ch sáº£n pháº©m"):
            if product_id.strip():
                try:
                    # Chuyá»ƒn Ä‘á»•i product_id thÃ nh sá»‘ nguyÃªn
                    product_id = int(product_id.strip())
                    product_reviews = valid_reviews[valid_reviews['ma_san_pham'] == product_id]
                    
                    if not product_reviews.empty:
                        st.write(f"PhÃ¢n tÃ­ch cho mÃ£ sáº£n pháº©m: {product_id}")
                        st.dataframe(product_reviews.head())
                        
                        # PhÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡
                        sentiment_counts = product_reviews['sentiment_rate'].value_counts()
                        
                        # PhÃ¢n loáº¡i Ä‘Ã¡nh giÃ¡
                        positive_reviews = " ".join(product_reviews[product_reviews['sentiment_rate'] == 'positive']['noi_dung_binh_luan_clean'].dropna())
                        neutral_reviews = " ".join(product_reviews[product_reviews['sentiment_rate'] == 'neutral']['noi_dung_binh_luan_clean'].dropna())
                        negative_reviews = " ".join(product_reviews[product_reviews['sentiment_rate'] == 'negative']['noi_dung_binh_luan_clean'].dropna())

                        # Váº½ WordCloud
                        st.write('### WordCloud cho sáº£n pháº©m:')
                        if positive_reviews:
                            wc_pos = WordCloud(background_color='white', max_words=100).generate(positive_reviews)
                            st.image(wc_pos.to_array(), caption="Positive Reviews WordCloud")
                        if neutral_reviews:
                            wc_neu = WordCloud(background_color='white', max_words=100).generate(neutral_reviews)
                            st.image(wc_neu.to_array(), caption="Neutral Reviews WordCloud")
                        if negative_reviews:
                            wc_neg = WordCloud(background_color='white', max_words=100).generate(negative_reviews)
                            st.image(wc_neg.to_array(), caption="Negative Reviews WordCloud")
                        
                        # Liá»‡t kÃª top tá»«
                        def get_top_keywords(text, top_n=10):
                            words = text.split()
                            word_counts = pd.Series(words).value_counts()
                            return word_counts.head(top_n).to_dict()

                        positive_keywords = get_top_keywords(positive_reviews)
                        neutral_keywords = get_top_keywords(neutral_reviews)
                        negative_keywords = get_top_keywords(negative_reviews)

                        st.write("### Tá»« khÃ³a phá»• biáº¿n nháº¥t:")
                        st.write("Top tá»« tÃ­ch cá»±c:", positive_keywords)
                        st.write("Top tá»« trung láº­p:", neutral_keywords)
                        st.write("Top tá»« tiÃªu cá»±c:", negative_keywords)

                        # Hiá»ƒn thá»‹ thÃ´ng tin thá»‘ng kÃª
                        st.write("### Thá»‘ng kÃª cáº£m xÃºc:")
                        st.write({
                            "MÃ£ sáº£n pháº©m": product_id,
                            "Tá»•ng Ä‘Ã¡nh giÃ¡ tÃ­ch cá»±c": sentiment_counts.get('positive', 0),
                            "Tá»•ng Ä‘Ã¡nh giÃ¡ trung láº­p": sentiment_counts.get('neutral', 0),
                            "Tá»•ng Ä‘Ã¡nh giÃ¡ tiÃªu cá»±c": sentiment_counts.get('negative', 0)
                        })
                    else:
                        st.warning(f"KhÃ´ng tÃ¬m tháº¥y Ä‘Ã¡nh giÃ¡ cho mÃ£ sáº£n pháº©m: {product_id}")
                except ValueError:
                    st.error("MÃ£ sáº£n pháº©m pháº£i lÃ  má»™t sá»‘ nguyÃªn há»£p lá»‡!")
            else:
                st.warning("Vui lÃ²ng nháº­p mÃ£ sáº£n pháº©m há»£p lá»‡!")

    elif choice == "Táº£i lÃªn táº­p CSV":
        st.subheader("Táº£i lÃªn tá»‡p CSV")
    
        # File uploader
        uploaded_file = st.file_uploader("Chá»n tá»‡p CSV cÃ³ cá»™t 'product_id'", type=["csv"])
    
        if uploaded_file is None:
            st.info("Vui lÃ²ng táº£i lÃªn tá»‡p CSV Ä‘á»ƒ báº¯t Ä‘áº§u phÃ¢n tÃ­ch.")
        else:
            try:
                # Äá»c file CSV
                data = pd.read_csv(uploaded_file)
    
                # Kiá»ƒm tra cá»™t 'product_id' cÃ³ tá»“n táº¡i
                if "product_id" not in data.columns:
                    st.error("Tá»‡p CSV pháº£i chá»©a cá»™t 'product_id'.")
                else:
                    # Xá»­ lÃ½ giÃ¡ trá»‹ trong cá»™t 'product_id'
                    data["product_id"] = pd.to_numeric(data["product_id"], errors="coerce")  # Chuyá»ƒn Ä‘á»•i cÃ¡c giÃ¡ trá»‹ há»£p lá»‡
                    data = data.dropna(subset=["product_id"])  # Loáº¡i bá» cÃ¡c hÃ ng cÃ³ giÃ¡ trá»‹ NaN
                    data["product_id"] = data["product_id"].astype(int)  # Chuyá»ƒn Ä‘á»•i thÃ nh sá»‘ nguyÃªn
    
                    # Láº¥y danh sÃ¡ch product_id duy nháº¥t
                    product_ids = data["product_id"].unique()
                    st.write(f"TÃ¬m tháº¥y {len(product_ids)} mÃ£ sáº£n pháº©m:")
                    st.write(product_ids)
    
                    # Káº¿t quáº£ phÃ¢n tÃ­ch sáº½ Ä‘Æ°á»£c lÆ°u á»Ÿ Ä‘Ã¢y
                    analysis_results = []
    
                    # PhÃ¢n tÃ­ch tá»«ng product ID
                    for product_id in product_ids:
                        st.subheader(f"PhÃ¢n tÃ­ch cho mÃ£ sáº£n pháº©m: {product_id}")
                        product_reviews = valid_reviews[valid_reviews['ma_san_pham'] == product_id]
                        if not product_reviews.empty:
                            st.dataframe(product_reviews.head())
    
                            # Tá»•ng sá»‘ Ä‘Ã¡nh giÃ¡
                            total_reviews = len(product_reviews)
                            st.write(f"Tá»•ng sá»‘ Ä‘Ã¡nh giÃ¡: {total_reviews}")
    
                            # PhÃ¢n phá»‘i cáº£m xÃºc
                            sentiment_counts = product_reviews['sentiment_rate'].value_counts()
                            st.write("PhÃ¢n phá»‘i cáº£m xÃºc:")
                            st.bar_chart(sentiment_counts)
    
                            # Táº¡o WordCloud vÃ  liá»‡t kÃª tá»« khÃ³a
                            sentiments = ["positive", "neutral", "negative"]
                            sentiment_text = {
                                sentiment: " ".join(
                                    product_reviews[product_reviews['sentiment_rate'] == sentiment]['noi_dung_binh_luan_clean']
                                    .dropna()
                                    .astype(str)
                                )
                                for sentiment in sentiments
                            }
    
                            for sentiment, text in sentiment_text.items():
                                if text.strip():  # Náº¿u cÃ³ dá»¯ liá»‡u
                                    # Táº¡o WordCloud
                                    wordcloud = WordCloud(background_color='white', max_words=100).generate(text)
                                    st.image(wordcloud.to_array(), caption=f"WordCloud - {sentiment.capitalize()}")
    
                                    # Liá»‡t kÃª tá»« khÃ³a chÃ­nh
                                    top_keywords = pd.Series(text.split()).value_counts().head(10)
                                    st.write(f"Top tá»« khÃ³a ({sentiment.capitalize()}):")
                                    st.write(top_keywords)
    
                            # LÆ°u káº¿t quáº£ phÃ¢n tÃ­ch
                            analysis_results.append({
                                "Product ID": product_id,
                                "Total Reviews": total_reviews,
                                "Positive Reviews": sentiment_counts.get("positive", 0),
                                "Neutral Reviews": sentiment_counts.get("neutral", 0),
                                "Negative Reviews": sentiment_counts.get("negative", 0),
                            })
                        else:
                            st.warning(f"KhÃ´ng tÃ¬m tháº¥y Ä‘Ã¡nh giÃ¡ cho mÃ£ sáº£n pháº©m: {product_id}")
    
                    # Táº£i xuá»‘ng káº¿t quáº£ phÃ¢n tÃ­ch
                    if analysis_results:
                        st.success("PhÃ¢n tÃ­ch hoÃ n táº¥t. Báº¡n cÃ³ thá»ƒ táº£i xuá»‘ng káº¿t quáº£.")
                        results_df = pd.DataFrame(analysis_results)
                        output = io.BytesIO()
                        results_df.to_csv(output, index=False)
                        output.seek(0)
    
                        st.download_button(
                            label="Táº£i xuá»‘ng káº¿t quáº£ phÃ¢n tÃ­ch",
                            data=output,
                            file_name="analysis_results.csv",
                            mime="text/csv",
                        )
            except Exception as e:
                st.error(f"Lá»—i khi xá»­ lÃ½ tá»‡p: {e}")
    
